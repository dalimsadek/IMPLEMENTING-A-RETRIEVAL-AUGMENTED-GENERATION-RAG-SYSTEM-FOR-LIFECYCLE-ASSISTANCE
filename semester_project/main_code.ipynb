{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "957e575b-1b8d-49f7-8e4a-3f787684bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07542689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536e7053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4434e86-ca80-407c-9272-e28b5807237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_arduino_code(code: str, sketch_name=\"MySketch\"):\n",
    "    folder = sketch_name\n",
    "    filename = f\"{sketch_name}.ino\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    file_path = os.path.join(folder, filename)\n",
    "\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(code)\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"arduino-cli\", \"compile\", \"--fqbn\", \"arduino:avr:uno\", folder],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        return False, \"arduino-cli not found or not in PATH\"\n",
    "\n",
    "    log = \"\\n\".join(result.stderr.splitlines()[:20]).strip() \n",
    "    return result.returncode == 0, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "424a95e8-2780-4b40-892b-9551b91d67f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files:  50%|█████     | 1/2 [34:45<34:45, 2085.80s/it]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "deepseek-ai/deepseek-coder-6.7b-instruct does not appear to have a file named model-00002-of-00002.safetensors. Checkout 'https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instruct/tree/main'for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer.pad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m      7\u001b[39m     tokenizer.pad_token = tokenizer.eos_token\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m code_fixer = TextGenerationPipeline(\n\u001b[32m     17\u001b[39m     model=model,\n\u001b[32m     18\u001b[39m     tokenizer=tokenizer,\n\u001b[32m     19\u001b[39m     return_full_text=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     20\u001b[39m     pad_token_id=tokenizer.pad_token_id\n\u001b[32m     21\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Moham\\OneDrive\\Bureau\\Eurecom\\Semester_Project\\tiny_ml\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Moham\\OneDrive\\Bureau\\Eurecom\\Semester_Project\\tiny_ml\\Lib\\site-packages\\transformers\\modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Moham\\OneDrive\\Bureau\\Eurecom\\Semester_Project\\tiny_ml\\Lib\\site-packages\\transformers\\modeling_utils.py:4260\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4250\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4251\u001b[39m     gguf_file\n\u001b[32m   4252\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4253\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map.values()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[32m   4254\u001b[39m ):\n\u001b[32m   4255\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   4256\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mloaded from GGUF files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4260\u001b[39m checkpoint_files, sharded_metadata = \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4262\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4263\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4265\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4267\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4273\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4276\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4278\u001b[39m is_sharded = sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4279\u001b[39m is_quantized = hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Moham\\OneDrive\\Bureau\\Eurecom\\Semester_Project\\tiny_ml\\Lib\\site-packages\\transformers\\modeling_utils.py:1152\u001b[39m, in \u001b[36m_get_resolved_checkpoint_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash)\u001b[39m\n\u001b[32m   1150\u001b[39m sharded_metadata = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[32m-> \u001b[39m\u001b[32m1152\u001b[39m     checkpoint_files, sharded_metadata = \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     checkpoint_files = [resolved_archive_file] \u001b[38;5;28;01mif\u001b[39;00m pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Moham\\OneDrive\\Bureau\\Eurecom\\Semester_Project\\tiny_ml\\Lib\\site-packages\\transformers\\utils\\hub.py:1115\u001b[39m, in \u001b[36mget_checkpoint_shard_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m   1111\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m shard_filenames, sharded_metadata\n\u001b[32m   1113\u001b[39m \u001b[38;5;66;03m# At this stage pretrained_model_name_or_path is a model identifier on the Hub. Try to get everything from cache,\u001b[39;00m\n\u001b[32m   1114\u001b[39m \u001b[38;5;66;03m# or download the files\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1115\u001b[39m cached_filenames = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshard_filenames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1124\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cached_filenames, sharded_metadata\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Moham\\OneDrive\\Bureau\\Eurecom\\Semester_Project\\tiny_ml\\Lib\\site-packages\\transformers\\utils\\hub.py:517\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    513\u001b[39m     revision_ = \u001b[33m\"\u001b[39m\u001b[33mmain\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m revision\n\u001b[32m    514\u001b[39m     msg = (\n\u001b[32m    515\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33ma file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_entries[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_entries) == \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfiles named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(*missing_entries,)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    516\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m    518\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not appear to have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Checkout \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/tree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    519\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfor available files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    520\u001b[39m     )\n\u001b[32m    522\u001b[39m \u001b[38;5;66;03m# Remove potential missing entries (we can silently remove them at this point based on the flags)\u001b[39;00m\n\u001b[32m    523\u001b[39m resolved_files = [file \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m resolved_files \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[31mOSError\u001b[39m: deepseek-ai/deepseek-coder-6.7b-instruct does not appear to have a file named model-00002-of-00002.safetensors. Checkout 'https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instruct/tree/main'for available files."
     ]
    }
   ],
   "source": [
    "# --- Load model ---\n",
    "from transformers import TextGenerationPipeline\n",
    "\n",
    "model_id = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "code_fixer = TextGenerationPipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e9616c-a793-4d5b-9e9c-d3f4774efbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_context_snippets(snippets, tokenizer, max_context_tokens):\n",
    "    combined = \"\"\n",
    "    for snippet in snippets:\n",
    "        temp = combined + \"\\n\\n\" + snippet\n",
    "        tokens = tokenizer(temp, return_tensors=\"pt\", truncation=False).input_ids\n",
    "        if tokens.shape[1] > max_context_tokens:\n",
    "            break\n",
    "        combined = temp\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fab392-d62b-4604-9a42-0c97f1d38fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_template import correct_code\n",
    "import re\n",
    "\n",
    "def extract_code_block(text):\n",
    "    match = re.search(r\"```(?:arduino)?\\s*(.*?)```\", text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return text.strip()  # fallback\n",
    "\n",
    "def fix_code(code, error, context_snippets, model, tokenizer, max_total_tokens=2048, max_new_tokens=256):\n",
    "    max_context_tokens = int(max_total_tokens * 0.75)\n",
    "    context = trim_context_snippets(context_snippets, tokenizer, max_context_tokens)\n",
    "\n",
    "    base_prompt = correct_code(context, code, error)  \n",
    "\n",
    "    input_ids = tokenizer(base_prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    if input_ids[\"input_ids\"].shape[1] + max_new_tokens > max_total_tokens:\n",
    "        allowed = max_total_tokens - max_new_tokens\n",
    "        input_ids[\"input_ids\"] = input_ids[\"input_ids\"][:, -allowed:]\n",
    "        input_ids[\"attention_mask\"] = input_ids[\"attention_mask\"][:, -allowed:]\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids[\"input_ids\"],\n",
    "        attention_mask=input_ids[\"attention_mask\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    decoded = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return extract_code_block(decoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2e7fa9-2317-471a-ba5d-7a31a7307668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_arduino_examples_from_repo(root_dir=\"arduino-examples/examples\"):\n",
    "    ino_files = []\n",
    "    for subdir, _, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".ino\"):\n",
    "                path = os.path.join(subdir, file)\n",
    "                try:\n",
    "                    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                        ino_files.append(f.read()[:5000])\n",
    "                except:\n",
    "                    pass\n",
    "    return ino_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515c44ee-c151-4027-b0f5-ac65ffd561c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_clean_code(output: str, fallback: str) -> str:\n",
    "    # Find the first ```arduino or ``` block\n",
    "    \n",
    "    match = re.search(r\"```(?:arduino)?\\s*(.*?)```\", output, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    \n",
    "    # Otherwise, try to grab from last 'Corrected Code:' onward\n",
    "    if \"Corrected Code:\" in output:\n",
    "        return output.split(\"Corrected Code:\")[-1].strip()\n",
    "\n",
    "    # Fallback if all else fails\n",
    "    return fallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e299fcb4-be9e-4104-be1d-bb3e978363d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def iterative_fix(original_code: str) -> str | None:\n",
    "    #code = original_code\n",
    "    #MAX_ATTEMPTS = 3\n",
    "    #docs, embedder, index = build_faiss_index_from_arduino_examples(\"arduino-examples/examples\")\n",
    "    #for attempt in range(1, MAX_ATTEMPTS + 1):\n",
    "    #ok, log = compile_arduino_code(code)\n",
    "    #if ok:\n",
    "        #print(f\"✅ Compiled on attempt {attempt}\")\n",
    "     #   return code\n",
    "\n",
    "    #print(f\"❌ Attempt {attempt} failed, retrying…\\n{log}\", file=sys.stderr)\n",
    "    #ctx = retrieve_context(log, embedder, index, docs)\n",
    "    #raw = fix_code(code, log, ctx, model, tokenizer)\n",
    "    #code = extract_clean_code(raw, code)\n",
    "    #print(\"❌ Exhausted retries — no working sketch.\")\n",
    "    #return raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fa4ade2-816f-4651-8be0-376935f9244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_fix(original_code: str) -> str:\n",
    "    code = original_code\n",
    "\n",
    "    # Try compiling\n",
    "    ok, log = compile_arduino_code(code)\n",
    "    if ok:\n",
    "        print(\"✅ Compilation succeeded — no correction needed.\")\n",
    "        return code\n",
    "\n",
    "    print(\"❌ Compilation failed. Attempting to fix…\\n\", log)\n",
    "\n",
    "    # Retrieve related Arduino examples\n",
    "    docs, embedder, index = build_faiss_index_from_arduino_examples(\"arduino-examples/examples\")\n",
    "    ctx = retrieve_context(log, embedder, index, docs)\n",
    "\n",
    "    # Generate a corrected version\n",
    "    raw = fix_code(code, log, ctx, model, tokenizer)\n",
    "\n",
    "    # Optional: clean the result to remove explanations or markdown\n",
    "    fixed_code = extract_clean_code(raw, fallback=code)\n",
    "\n",
    "    return fixed_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7932a94b-fdcb-4398-adbf-0eb6d8c81774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_faiss_index_from_arduino_examples(local_path=\"arduino-examples/examples\"):\n",
    "    docs = load_arduino_examples_from_repo(local_path)\n",
    "    embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    embeddings = embedder.encode(docs).astype(\"float32\") \n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(np.array(embeddings))\n",
    "    return docs, embedder, index\n",
    "\n",
    "def retrieve_context(query, embedder, index, docs, top_k=2):\n",
    "    q_vec = np.array(embedder.encode([query]))\n",
    "    D, I = index.search(q_vec, top_k)\n",
    "    return [docs[i] for i in I[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f80edc3c-24d3-4c3e-b7f7-0fb7c30a3459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Compilation failed. Attempting to fix…\n",
      " C:\\Users\\medaminekh\\semester_project\\MySketch\\MySketch.ino: In function 'void setup()':\n",
      "C:\\Users\\medaminekh\\semester_project\\MySketch\\MySketch.ino:3:3: error: expected ';' before 'pinMode'\n",
      "   pinMode(13, OUTPUT);  // missing comma between arguments\n",
      "   ^~~~~~~\n",
      "Error during build: exit status 1\n",
      "\n",
      "—— Final working sketch ——\n",
      "\n",
      "void setup() {\n",
      "  Serial.begin(9600);\n",
      "  pinMode(13, OUTPUT);  // added comma between arguments\n",
      "}\n",
      "\n",
      "void loop() {\n",
      "  digitalWrite(13, HIGH);\n",
      "  delay(1000);\n",
      "}\n",
      "\n",
      "\n",
      "### Explanation:\n",
      "The error message indicates that the compiler is expecting a ';' after 'pinMode' but it's not. The missing comma after '13' in 'pinMode(13, OUTPUT);' is causing the error.\n",
      "\n",
      "The corrected code adds a comma after '13' in 'pinMode(13, OUTPUT);' to fix the error. The 'HIGH' value is also added after 'digitalWrite(13, \"HIGH\");' to fix the error.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "buggy_code_path = Path(\"buggy_code.ino\")\n",
    "buggy_code = buggy_code_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "fixed = iterative_fix(buggy_code)\n",
    "if fixed:\n",
    "    print(\"\\n—— Final working sketch ——\\n\")\n",
    "    print(fixed)\n",
    "else:\n",
    "    print(\"No valid fix found after retries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4748b1d4-1a34-4902-8bde-71465ae36658",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiny_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
