{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6ba74f5",
   "metadata": {},
   "source": [
    "# Generate Data Preprocessing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "429dfbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from context_builder import ContextBuilder\n",
    "from data_preprocessor import DataPreprocessor\n",
    "\n",
    "builder = ContextBuilder(\n",
    "    csv_path=\"gesture_recognition.csv\",\n",
    "    task_description=\"Classify between two gestures: punch and flex, based on IMU readings from Arduino Nano 33 BLE Sense.\",\n",
    "    feature_description={\n",
    "        \"aX\": \"Accelerometer X-axis\",\n",
    "        \"aY\": \"Accelerometer Y-axis\",\n",
    "        \"aZ\": \"Accelerometer Z-axis\",\n",
    "        \"gX\": \"Gyroscope X-axis\",\n",
    "        \"gY\": \"Gyroscope Y-axis\",\n",
    "        \"gZ\": \"Gyroscope Z-axis\"\n",
    "    },\n",
    "    label_description=\"The 'label' column indicates the gesture performed. Possible values: 'punch', 'flex'.\",\n",
    "    notes=\"The IMU readings were captured from the LSM9DS1 sensor at ~100Hz. Each row represents a single gesture frame.\"\n",
    ")\n",
    "\n",
    "context = builder.get_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f0b90247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RetrieverBuilder] Loading Chroma from: chroma_db\n",
      "[RetrieverBuilder] Chroma loaded. Ready to retrieve top-4 documents using cosine similarity.\n"
     ]
    }
   ],
   "source": [
    "from retriever_instance import RetrieverBuilder\n",
    "builder = RetrieverBuilder()\n",
    "#builder.ingest_json_chunks([\"converted_data.json\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7cfac10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"{context['task']} {context['notes']}\"\n",
    "retriever = builder.get_retriever()\n",
    "chunks = retriever.get_relevant_documents(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2e4176a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'examples//lite//examples\\\\gesture_classification\\\\ios\\\\README.md', 'chunk_id': '1-3'}, page_content='gestures you trained in step 4 and the app identifies them in realtime! ## Model Used This app uses [MobileNet](https://ai.googleblog.com/2017/06/mobilenets-open-source-models-for.html) model that is trained on 0.25 alpha and at an image size'),\n",
       " Document(metadata={'chunk_id': '1-3', 'source': 'examples//lite//examples\\\\gesture_classification\\\\ios\\\\README.md'}, page_content='gestures you trained in step 4 and the app identifies them in realtime! ## Model Used This app uses [MobileNet](https://ai.googleblog.com/2017/06/mobilenets-open-source-models-for.html) model that is trained on 0.25 alpha and at an image size'),\n",
       " Document(metadata={'source': 'examples//lite//examples\\\\gesture_classification\\\\ios\\\\README.md', 'chunk_id': '1-3'}, page_content='gestures you trained in step 4 and the app identifies them in realtime! ## Model Used This app uses [MobileNet](https://ai.googleblog.com/2017/06/mobilenets-open-source-models-for.html) model that is trained on 0.25 alpha and at an image size'),\n",
       " Document(metadata={'chunk_id': 0, 'source': 'examples//lite//examples\\\\gesture_classification\\\\README.md'}, page_content='# Gesture Classification ## Overview This directory contains end-to-end samples that perform gesture classification on live camera feed. * An [Android app](android/) that uses the TensorFlow Lite model to classify the gesture on camera. * An [iOS app](ios/) that uses the TensorFlow Lite model to classify the gesture on camera. * A [web app](web/) that uses the TensorFlow JS model to predict the gesture from a webcam. * A [guide](ml/) to get TFJS model and TFLite model.')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "10d478ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLMWrapper] Initialized Groq LLM: llama3-70b-8192 (temp=0.3)\n",
      "[RetrieverBuilder] Loading Chroma from: chroma_db\n",
      "[RetrieverBuilder] Chroma loaded. Ready to retrieve top-5 documents using cosine similarity.\n"
     ]
    }
   ],
   "source": [
    "preprocessor = DataPreprocessor(context)\n",
    "task_suggestions = preprocessor.suggest_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2dd748a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLMWrapper] Initialized Groq LLM: llama3-70b-8192 (temp=0.3)\n"
     ]
    }
   ],
   "source": [
    "from data_preprocessor import PreprocessingCodeGenerator\n",
    "codegen = PreprocessingCodeGenerator(context, task_suggestions, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "19874da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# === HANDLE_UNNAMED_COLUMN ===\n",
      "content=\"df.drop('Unnamed: 0', axis=1, inplace=True)\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 833, 'total_tokens': 849, 'completion_time': 0.07041316, 'prompt_time': 0.034952327, 'queue_time': 0.093363054, 'total_time': 0.105365487}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None} id='run-a5011a5b-442f-455a-8c5d-440054ffe7a7-0' usage_metadata={'input_tokens': 833, 'output_tokens': 16, 'total_tokens': 849}\n",
      "\n",
      "# === DROP_UNNECESSARY_FEATURES ===\n",
      "content=\"def drop_unnecessary_features(df):\\n    df = df.drop(['Unnamed: 0'], axis=1)\\n    return df\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 834, 'total_tokens': 860, 'completion_time': 0.080690938, 'prompt_time': 0.034916978, 'queue_time': 0.09315692799999999, 'total_time': 0.115607916}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None} id='run-8efe6d74-a589-4457-9d5e-f3eed34a5548-0' usage_metadata={'input_tokens': 834, 'output_tokens': 26, 'total_tokens': 860}\n",
      "\n",
      "# === SCALE_IMU_READINGS ===\n",
      "content=\"from sklearn.preprocessing import StandardScaler\\n\\nscaler = StandardScaler()\\n\\ndf[['aX', 'aY', 'aZ', 'gX', 'gY', 'gZ']] = scaler.fit_transform(df[['aX', 'aY', 'aZ', 'gX', 'gY', 'gZ']])\\n\\nprint(df.head())\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 73, 'prompt_tokens': 834, 'total_tokens': 907, 'completion_time': 0.208571429, 'prompt_time': 0.034988677, 'queue_time': 0.09647345500000001, 'total_time': 0.243560106}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None} id='run-045b04c6-8ef2-46fc-bebd-739e706bbe4a-0' usage_metadata={'input_tokens': 834, 'output_tokens': 73, 'total_tokens': 907}\n",
      "\n",
      "# === SPLIT_DATA_INTO_TRAIN_AND_TEST ===\n",
      "content=\"from sklearn.model_selection import train_test_split\\n\\ndef Split_Data_into_Train_and_Test(df):\\n    X = df.drop(['label', 'Unnamed: 0'], axis=1)\\n    y = df['label']\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n    return X_train, X_test, y_train, y_test\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 90, 'prompt_tokens': 836, 'total_tokens': 926, 'completion_time': 0.257142857, 'prompt_time': 0.034864869, 'queue_time': 0.095209495, 'total_time': 0.292007726}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None} id='run-78a7d064-c192-40b7-94a8-692ddda7cdaf-0' usage_metadata={'input_tokens': 836, 'output_tokens': 90, 'total_tokens': 926}\n",
      "\n",
      "# === HANDLE_CLASS_IMBALANCE ===\n",
      "content=\"from imblearn.over_sampling\\nfrom imblearn.pipeline\\nimport pandas as pd\\n\\n# Separate features and labels\\nX = df.drop(['label'], axis=1)\\ny = df['label']\\n\\n# Create a pipeline with oversampling\\noversample = over_sampling.RandomOverSampler(sampling_strategy='auto', random_state=42)\\npipeline = pipeline.Pipeline([\\n    ('oversample', oversample)\\n])\\n\\n# Fit and transform the data\\nX_res, y_res = pipeline.fit_resample(X, y)\\n\\n# Create a new dataframe with the resampled data\\ndf_res = pd.concat([X_res, y_res], axis=1)\\n\\nprint(df_res.head())\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 137, 'prompt_tokens': 834, 'total_tokens': 971, 'completion_time': 0.412234619, 'prompt_time': 0.035002586, 'queue_time': 0.09457278, 'total_time': 0.447237205}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None} id='run-bd08ac64-9a04-4944-aaa4-0f60dba36cfa-0' usage_metadata={'input_tokens': 834, 'output_tokens': 137, 'total_tokens': 971}\n",
      "\n",
      "# === FEATURE_ENGINEERING_FOR_IMU_DATA ===\n",
      "content=\"```\\ndef feature_engineering_for_imu_data(df):\\n    # Drop the 'Unnamed: 0' column\\n    df.drop('Unnamed: 0', axis=1, inplace=True)\\n    \\n    # Calculate the magnitude of acceleration and gyroscope\\n    df['aMagnitude'] = (df['aX']**2 + df['aY']**2 + df['aZ']**2)**0.5\\n    df['gMagnitude'] = (df['gX']**2 + df['gY']**2 + df['gZ']**2)**0.5\\n    \\n    # Calculate the mean and standard deviation of acceleration and gyroscope\\n    df['aMeanX'] = df['aX'].rolling(window=10).mean()\\n    df['aMeanY'] = df['aY'].rolling(window=10).mean()\\n    df['aMeanZ'] = df['aZ'].rolling(window=10).mean()\\n    df['aStdX'] = df['aY'].rolling(window=10).std()\\n    df['aStdY'] = df['aY'].rolling(window=10).std()\\n    df['aStdZ'] = df['aZ'].rolling(window=10).std()\\n    \\n    df['gMeanX'] = df['gX'].rolling(window=10).mean()\\n    df['gMeanY'] = df['gY'].rolling(window=10).mean()\\n    df['gMeanZ'] = df['gZ'].rolling(window=10).mean()\\n    df['gStdX'] = df['gX'].rolling(window=10).std()\\n    df['gStdY'] = df['gY'].rolling(window=10).std()\\n    df['gStdZ'] = df['gZ'].rolling(window=10).std()\\n    \\n    return df\\n```\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 386, 'prompt_tokens': 837, 'total_tokens': 1223, 'completion_time': 1.102857143, 'prompt_time': 0.027299549, 'queue_time': 0.11251715299999998, 'total_time': 1.130156692}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None} id='run-ee50ee45-7ef5-4fe1-809e-d1a213f1f4f0-0' usage_metadata={'input_tokens': 837, 'output_tokens': 386, 'total_tokens': 1223}\n",
      "\n",
      "# === CONVERT_LABELS_TO_NUMERICAL_VALUES ===\n",
      "content=\"```\\ndef Convert_Labels_to_Numerical_Values(df):\\n    df['label'] = df['label'].map({'punch': 0, 'flex': 1})\\n    return df\\n```\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 838, 'total_tokens': 882, 'completion_time': 0.136429952, 'prompt_time': 0.027412237, 'queue_time': 0.094036486, 'total_time': 0.163842189}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None} id='run-0d052850-65b7-4be6-bf5e-853e340e5d4f-0' usage_metadata={'input_tokens': 838, 'output_tokens': 44, 'total_tokens': 882}\n"
     ]
    }
   ],
   "source": [
    "for task, description in task_suggestions.items():\n",
    "    print(f\"\\n# === {task.upper()} ===\")\n",
    "    print(codegen.generate_code(task, description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8d76e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f742a959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import merger\n",
    "from merger import CodeMergerAndFixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f7a20bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLMWrapper] Initialized Groq LLM: llama3-70b-8192 (temp=0.2)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Collect generated codes\n",
    "task_code_blocks = {}\n",
    "for task, description in task_suggestions.items():\n",
    "    code = codegen.generate_code(task, description)\n",
    "    if hasattr(code, \"content\"):\n",
    "        code = code.content\n",
    "    task_code_blocks[task] = code\n",
    "\n",
    "# Step 2: Merge and fix using LLM\n",
    "merger = CodeMergerAndFixer(context , task_code_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "03cfc58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_code = merger.merge_with_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d003695a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from imblearn.over_sampling import RandomOverSampler\n",
      "from imblearn.pipeline import Pipeline\n",
      "\n",
      "df = pd.read_csv('gesture_recognition.csv')\n",
      "\n",
      "df = df.rename(columns={'Unnamed: 0': 'frame_id'})\n",
      "df = df.drop_duplicates(subset='frame_id')\n",
      "\n",
      "df = df.drop(['Unnamed: 0'], axis=1)\n",
      "\n",
      "scaler = StandardScaler()\n",
      "df[['aX', 'aY', 'aZ', 'gX', 'gY', 'gZ']] = scaler.fit_transform(df[['aX', 'aY', 'aZ', 'gX', 'gY', 'gZ']])\n",
      "\n",
      "df = df.drop('Unnamed: 0', axis=1)\n",
      "y = df['label']\n",
      "\n",
      "oversample = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
      "pipeline = Pipeline([('oversample', oversample)])\n",
      "pipeline.fit_resample(X, y)\n",
      "\n",
      "df['aMagnitude'] = (df['aX']**2 + df['aY']**2 + df['aZ']**2)**0.5\n",
      "df['gMagnitude'] = (df['gX']**2 + df['gY']**2 + df['gZ']**2)**0.5\n",
      "\n",
      "df['aMeanX'] = df['aX'].rolling(window=10).mean()\n",
      "df['aMeanY'] = df['aY'].rolling(window=10).mean()\n",
      "df['aMeanZ'] = df['aZ'].rolling(window=10).mean()\n",
      "df['aStdX'] = df['aX'].rolling(window=10).std()\n",
      "df['aStdY'] = df['aY'].rolling(window=10).std()\n",
      "df['aStdZ'] = df['aZ'].rolling(window=10).std()\n",
      "\n",
      "df['gMeanX'] = df['gX'].rolling(window=10).mean()\n",
      "df['gMeanY'] = df['gY'].rolling(window=10).mean()\n",
      "df['gMeanZ'] = df['gZ'].rolling(window=10).mean()\n",
      "df['gStdX'] = df['gX'].rolling(window=10).std()\n",
      "df['gStdY'] = df['gY'].rolling(window=10).std()\n",
      "df['gStdZ'] = df['gZ'].rolling(window=10).std()\n",
      "\n",
      "df['label'] = df['label'].map({'punch': 0, 'flex': 1})\n",
      "\n",
      "X = df.drop(['label', 'frame_id'], axis=1)\n",
      "y = df['label']\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "print(full_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e8f61e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import auto_code_executor\n",
    "importlib.reload(auto_code_executor)\n",
    "from auto_code_executor import AutoCodeExecutor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f9787245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLMWrapper] Initialized Groq LLM: llama3-70b-8192 (temp=0.2)\n"
     ]
    }
   ],
   "source": [
    "executor = AutoCodeExecutor(full_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8bb3f491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛠 Attempt 1...\n",
      "🔎 Current Code Preview:\n",
      " import pandas as pd\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from imblearn.over_sampling import RandomOverSampler\n",
      "from imblearn.pipeline import Pipeline\n",
      "\n",
      "df = pd.read_csv('gesture_recognition.csv')\n",
      "\n",
      "df = df.rename(columns={'Unnamed: 0': 'f\n",
      "❌ Error detected: \"['Unnamed: 0'] not found in axis\"\n",
      "🔧 LLM generated a fixed version!\n",
      "🛠 Attempt 2...\n",
      "🔎 Current Code Preview:\n",
      " ```\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from imblearn.over_sampling import RandomOverSampler\n",
      "from imblearn.pipeline import Pipeline\n",
      "\n",
      "df = pd.read_csv('gesture_recognition.csv')\n",
      "\n",
      "df = df.rename(columns={'Unnamed: 0'\n",
      "❌ Error detected: \"['Unnamed: 0'] not found in axis\"\n",
      "🔧 LLM generated a fixed version!\n",
      "🛠 Attempt 3...\n",
      "🔎 Current Code Preview:\n",
      " ```\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from imblearn.over_sampling import RandomOverSampler\n",
      "from imblearn.pipeline import Pipeline\n",
      "\n",
      "df = pd.read_csv('gesture_recognition.csv')\n",
      "\n",
      "df = df.rename(columns={'Unnamed: 0'\n",
      "✅ Code executed successfully!\n"
     ]
    }
   ],
   "source": [
    "success = executor.try_execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87a8058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01f7bfa4",
   "metadata": {},
   "source": [
    "## Data Preprocesing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dcc4a053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tools' from 'c:\\\\Users\\\\Moham\\\\OneDrive\\\\Bureau\\\\Eurecom\\\\Semester_Project\\\\src\\\\tools.py'>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import tools\n",
    "importlib.reload(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cf1c4b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import (\n",
    "    load_context,          # 1️⃣\n",
    "    suggest_preprocessing, # 2️⃣\n",
    "    generate_code,         # 3️⃣\n",
    "    merge_snippets,        # 4️⃣\n",
    "    run_pipeline_from_file       # 5️⃣\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "789fbf24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Classify IMU frames into 'punch' vs 'flex'\n",
      "Features:\n",
      "  - aX: acc-X\n",
      "  - aY: acc-Y\n",
      "  - aZ: acc-Z\n",
      "  - gX: gyro-X\n",
      "  - gY: gyro-Y\n",
      "  - gZ: gyro-Z\n",
      "Label Info: gesture: 0 = punch, 1 = flex\n",
      "Notes: LSM9DS1 sensor (~100 Hz). One row = one frame.\n",
      "Sample Data:\n",
      "  - {'Unnamed: 0': 0, 'aX': 0.067, 'aY': 0.904, 'aZ': 2.018, 'gX': 70.435, 'gY': -10.315, 'gZ': 1.892, 'label': 'punch'}\n",
      "  - {'Unnamed: 0': 1, 'aX': -0.045, 'aY': 1.269, 'aZ': 2.24, 'gX': 83.191, 'gY': -11.292, 'gZ': 9.583, 'label': 'punch'}\n",
      "  - {'Unnamed: 0': 2, 'aX': -0.276, 'aY': 1.609, 'aZ': 2.364, 'gX': 94.849, 'gY': -5.249, 'gZ': 12.451, 'label': 'punch'}\n",
      "  - {'Unnamed: 0': 3, 'aX': -0.564, 'aY': 2.0, 'aZ': 2.359, 'gX': 111.023, 'gY': 2.625, 'gZ': 7.874, 'label': 'punch'}\n",
      "  - {'Unnamed: 0': 4, 'aX': -0.845, 'aY': 2.311, 'aZ': 2.335, 'gX': 137.878, 'gY': 10.681, 'gZ': -2.197, 'label': 'punch'}\n"
     ]
    }
   ],
   "source": [
    "context_summary = load_context.invoke({\n",
    "    \"csv_path\":          \"gesture_recognition.csv\",\n",
    "    \"task_description\":  \"Classify IMU frames into 'punch' vs 'flex'\",\n",
    "    \"feature_desc_json\": '{\"aX\":\"acc-X\",\"aY\":\"acc-Y\",\"aZ\":\"acc-Z\",\"gX\":\"gyro-X\",\"gY\":\"gyro-Y\",\"gZ\":\"gyro-Z\"}',\n",
    "    \"label_desc\":        \"gesture: 0 = punch, 1 = flex\",\n",
    "    \"notes\":             \"LSM9DS1 sensor (~100 Hz). One row = one frame.\"\n",
    "})\n",
    "print(context_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "50526d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLMWrapper] Initialized Groq LLM: llama3-70b-8192 (temp=0.3)\n",
      "[RetrieverBuilder] Loading Chroma from: chroma_db\n",
      "[RetrieverBuilder] Chroma loaded. Ready to retrieve top-4 documents using cosine similarity.\n"
     ]
    }
   ],
   "source": [
    "suggest_json = suggest_preprocessing.invoke({\"top_k\":4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc7764df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Drop Unnamed Column\": \"\",\n",
      "  \"Convert Label to Numeric\": \"\",\n",
      "  \"Handle Missing Values\": \"\",\n",
      "  \"Feature Scaling\": \"\",\n",
      "  \"Data Split\": \"\",\n",
      "  \"Data Augmentation\": \"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(suggest_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e3b0a223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generating snippet for →  Drop Unnamed Column\n",
      "[LLMWrapper] Initialized Groq LLM: llama3-70b-8192 (temp=0.3)\n",
      "[Drop Unnamed Column] snippet stored (533 chars):\n",
      "content=\"df.drop('Unnamed: 0', axis=1, inplace=True)\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 667, 'total_tokens': 683, 'completion_time': 0.071793043, 'prompt_ti …\n",
      "[LLMWrapper] Initialized Groq LLM: llama3-70b-8192 (temp=0.2)\n",
      "Merged 1 snippets → generated_code\\merged_pipeline.py\n",
      "\n",
      "### Generating snippet for →  Convert Label to Numerical\n",
      "[LLMWrapper] Initialized Groq LLM: llama3-70b-8192 (temp=0.3)\n",
      "[Convert Label to Numerical] snippet stored (623 chars):\n",
      "content=\"Here is the Python code to convert the label to numerical:\\n```\\ndf['label'] = df['label'].map({'punch': 0, 'flex': 1})\\n```\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 39, 'pro …\n",
      "[LLMWrapper] Initialized Groq LLM: llama3-70b-8192 (temp=0.2)\n",
      "Merged 2 snippets → generated_code\\merged_pipeline.py\n",
      "\n",
      "### Generating snippet for →  Handle Missing Values\n",
      "[LLMWrapper] Initialized Groq LLM: llama3-70b-8192 (temp=0.3)\n",
      "[Handle Missing Values] snippet stored (1138 chars):\n",
      "content=\"Here is the Python code to handle missing values in the dataset:\\n```\\nimport pandas as pd\\nimport numpy as np\\n\\n# Assuming the dataset is in a variable named df\\ndf = pd.DataFrame([...])  # replace with your d …\n",
      "[LLMWrapper] Initialized Groq LLM: llama3-70b-8192 (temp=0.2)\n",
      "Merged 3 snippets → generated_code\\merged_pipeline.py\n",
      "\n",
      "### Generating snippet for →  Normalize Features\n",
      "[LLMWrapper] Initialized Groq LLM: llama3-70b-8192 (temp=0.3)\n",
      "[Normalize Features] snippet stored (683 chars):\n",
      "content=\"from sklearn.preprocessing import StandardScaler\\n\\nscaler = StandardScaler()\\ndf[['aX', 'aY', 'aZ', 'gX', 'gY', 'gZ']] = scaler.fit_transform(df[['aX', 'aY', 'aZ', 'gX', 'gY', 'gZ']])\" additional_kwargs={} resp …\n",
      "[LLMWrapper] Initialized Groq LLM: llama3-70b-8192 (temp=0.2)\n",
      "Merged 4 snippets → generated_code\\merged_pipeline.py\n",
      "\n",
      "### Generating snippet for →  Feature Engineering (e.g. calculating derivatives)\n",
      "[LLMWrapper] Initialized Groq LLM: llama3-70b-8192 (temp=0.3)\n",
      "[Feature Engineering (e.g. calculating derivatives)] snippet stored (733 chars):\n",
      "content=\"```\\ndf['aX_derivative'] = df['aX'].diff()\\ndf['aY_derivative'] = df['aY'].diff()\\ndf['aZ_derivative'] = df['aZ'].diff()\\ndf['gX_derivative'] = df['gX'].diff()\\ndf['gY_derivative'] = df['gY'].diff()\\ndf['gZ_deri …\n",
      "[LLMWrapper] Initialized Groq LLM: llama3-70b-8192 (temp=0.2)\n",
      "Merged 5 snippets → generated_code\\merged_pipeline.py\n",
      "\n",
      "### Generating snippet for →  Split Data into Training and Testing Sets\n",
      "[LLMWrapper] Initialized Groq LLM: llama3-70b-8192 (temp=0.3)\n",
      "[Split Data into Training and Testing Sets] snippet stored (804 chars):\n",
      "content=\"from sklearn.model_selection import train_test_split\\n\\nX = df.drop(['Unnamed: 0', 'label'], inplace=False)\\ny = df['label'].map({'punch': 0, 'flex': 1})\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, …\n",
      "[LLMWrapper] Initialized Groq LLM: llama3-70b-8192 (temp=0.2)\n",
      "Merged 6 snippets → generated_code\\merged_pipeline.py\n",
      "\n",
      "### Generating snippet for →  Data Augmentation (e.g. adding noise to IMU data)\n",
      "[LLMWrapper] Initialized Groq LLM: llama3-70b-8192 (temp=0.3)\n",
      "[Data Augmentation (e.g. adding noise to IMU data)] snippet stored (1093 chars):\n",
      "content=\"Here is the Python code to perform data augmentation by adding noise to the IMU data:\\n```\\nimport pandas as pd\\nimport numpy as np\\n\\n# Define a function to add noise to a single column\\ndef add_noise(col, nois …\n",
      "[LLMWrapper] Initialized Groq LLM: llama3-70b-8192 (temp=0.2)\n",
      "Merged 7 snippets → generated_code\\merged_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "import json, ast\n",
    "task_order = list(json.loads(suggest_json))   # keep original order\n",
    "success    = True\n",
    "for task in task_order:\n",
    "    print(f\"\\n### Generating snippet for →  {task}\")\n",
    "    snippet_preview = generate_code(task)            # (tool stores snippet)\n",
    "    print(snippet_preview)                           # first 220 chars\n",
    "\n",
    "    # merge *all* snippets collected so far\n",
    "    merge_msg = merge_snippets(\"\")                   # dummy arg required\n",
    "    print(merge_msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25e2dfa",
   "metadata": {},
   "source": [
    "### Run the merged_pipeline.py to be viewed by the preprocess module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf56572e",
   "metadata": {},
   "source": [
    "## Generate Preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cefb72bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Moham\\OneDrive\\Bureau\\Eurecom\\Semester_Project\\src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"src\")\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "354d49f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generated_code/merged_pipeline.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9331a5a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'add_noise' from 'generated_code.merged_pipeline' (c:\\Users\\Moham\\OneDrive\\Bureau\\Eurecom\\Semester_Project\\src\\generated_code\\merged_pipeline.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[101]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgenerated_code\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmerged_pipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m add_noise\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'add_noise' from 'generated_code.merged_pipeline' (c:\\Users\\Moham\\OneDrive\\Bureau\\Eurecom\\Semester_Project\\src\\generated_code\\merged_pipeline.py)"
     ]
    }
   ],
   "source": [
    "from generated_code.merged_pipeline import add_noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "98020e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "# Add src/ to sys.path so 'generated_code' is importable\n",
    "sys.path.insert(0, os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadba043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLMWrapper] Initialized Groq LLM: llama3-70b-8192 (temp=0.1)\n",
      "[LLMWrapper] Initialized Groq LLM: llama3-70b-8192 (temp=0.2)\n",
      "🛠 Attempt 1...\n",
      "🔎 Current Code Preview:\n",
      " import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "```\n",
      "from generated_code.merged_pipeline import add_noise\n",
      "\n",
      "def preprocess(df):\n",
      "    df.drop('Unnamed: 0, axis=1, inplace=True)\n",
      "    df['label'] = d\n",
      "❌ Error detected: unterminated string literal (detected at line 10) (<string>, line 10)\n",
      "🔁 LLM fixed preprocess function using external imports.\n",
      "🛠 Attempt 2...\n",
      "🔎 Current Code Preview:\n",
      " Here is the corrected Python code:\n",
      "\n",
      "```\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from generated_code.merged_pipeline import add_noise\n",
      "\n",
      "def preprocess(df):\n",
      "    df.drop(columns='Unnamed: 0\n",
      "❌ Error detected: invalid syntax (<string>, line 2)\n",
      "🔁 LLM fixed preprocess function using external imports.\n",
      "🛠 Attempt 3...\n",
      "🔎 Current Code Preview:\n",
      " ```\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from generated_code.merged_pipeline import add_noise\n",
      "\n",
      "def preprocess(df):\n",
      "    df.drop(columns='Unnamed: 0', inplace=True)\n",
      "    df['label'] = d\n",
      "❌ Error detected: cannot import name 'add_noise' from 'generated_code.merged_pipeline' (c:\\Users\\Moham\\OneDrive\\Bureau\\Eurecom\\Semester_Project\\src\\generated_code\\merged_pipeline.py)\n",
      "🔁 LLM fixed preprocess function using external imports.\n",
      "🛠 Attempt 4...\n",
      "🔎 Current Code Preview:\n",
      " ```\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from generated_code.merged_pipeline import add_noise, diff, fit_transform, map, read_csv, reset_index, train_test_split\n",
      "\n",
      "def preprocess(df):\n",
      "\n",
      "❌ Error detected: cannot import name 'add_noise' from 'generated_code.merged_pipeline' (c:\\Users\\Moham\\OneDrive\\Bureau\\Eurecom\\Semester_Project\\src\\generated_code\\merged_pipeline.py)\n",
      "🔁 LLM fixed preprocess function using external imports.\n",
      "🛠 Attempt 5...\n",
      "🔎 Current Code Preview:\n",
      " Here is the corrected Python code:\n",
      "\n",
      "```\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from generated_code.merged_pipeline import add_noise, train_test_split\n",
      "\n",
      "def preprocess(df):\n",
      "    df.drop(c\n",
      "❌ Error detected: invalid syntax (<string>, line 2)\n",
      "🔁 LLM fixed preprocess function using external imports.\n",
      "🛠 Attempt 6...\n",
      "🔎 Current Code Preview:\n",
      " ```\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from generated_code.merged_pipeline import add_noise\n",
      "\n",
      "def preprocess(df):\n",
      "    df.drop(columns='Unnamed: 0', inplace=True)\n",
      "    df['label'] = d\n",
      "❌ Error detected: cannot import name 'add_noise' from 'generated_code.merged_pipeline' (c:\\Users\\Moham\\OneDrive\\Bureau\\Eurecom\\Semester_Project\\src\\generated_code\\merged_pipeline.py)\n",
      "🔁 LLM fixed preprocess function using external imports.\n",
      "🛠 Attempt 7...\n",
      "🔎 Current Code Preview:\n",
      " ```\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from generated_code.merged_pipeline import add_noise\n",
      "\n",
      "def preprocess(df):\n",
      "    df.drop(columns='Unnamed: 0', inplace=True)\n",
      "    df['label'] = d\n",
      "❌ Error detected: cannot import name 'add_noise' from 'generated_code.merged_pipeline' (c:\\Users\\Moham\\OneDrive\\Bureau\\Eurecom\\Semester_Project\\src\\generated_code\\merged_pipeline.py)\n",
      "🔁 LLM fixed preprocess function using external imports.\n",
      "🛠 Attempt 8...\n",
      "🔎 Current Code Preview:\n",
      " ```\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from generated_code.merged_pipeline import add_noise as add_noise_function\n",
      "\n",
      "def preprocess(df):\n",
      "    df.drop(columns='Unnamed: 0', inplace=Tru\n",
      "❌ Error detected: cannot import name 'add_noise' from 'generated_code.merged_pipeline' (c:\\Users\\Moham\\OneDrive\\Bureau\\Eurecom\\Semester_Project\\src\\generated_code\\merged_pipeline.py)\n",
      "🔁 LLM fixed preprocess function using external imports.\n",
      "🛠 Attempt 9...\n",
      "🔎 Current Code Preview:\n",
      " ```\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from generated_code.merged_pipeline import add_noise\n",
      "\n",
      "def preprocess(df):\n",
      "    df.drop(columns='Unnamed: 0', inplace=True)\n",
      "    df['label'] = d\n",
      "❌ Error detected: cannot import name 'add_noise' from 'generated_code.merged_pipeline' (c:\\Users\\Moham\\OneDrive\\Bureau\\Eurecom\\Semester_Project\\src\\generated_code\\merged_pipeline.py)\n",
      "🔁 LLM fixed preprocess function using external imports.\n",
      "🛠 Attempt 10...\n",
      "🔎 Current Code Preview:\n",
      " ```\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "from generated_code.merged_pipeline import add_noise\n",
      "\n",
      "def preprocess(df):\n",
      "    df.drop(columns='Unnamed: 0', inplace=True)\n",
      "    df['label'] = d\n",
      "❌ Error detected: cannot import name 'add_noise' from 'generated_code.merged_pipeline' (c:\\Users\\Moham\\OneDrive\\Bureau\\Eurecom\\Semester_Project\\src\\generated_code\\merged_pipeline.py)\n",
      "🔁 LLM fixed preprocess function using external imports.\n",
      "❌ Failed to fix preprocess function after maximum attempts.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'❌ Code generation failed after auto-fixing attempts. Final version saved.'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import tools\n",
    "importlib.reload(tools)\n",
    "import auto_code_executor\n",
    "importlib.reload(auto_code_executor)\n",
    "importlib.reload(PreprocessFunctionBuilder)\n",
    "\n",
    "import generated_code.merged_pipeline\n",
    "from data_preprocessor import PreprocessFunctionBuilder\n",
    "\n",
    "from tools import build_preprocess_module\n",
    "\n",
    "build_preprocess_module.invoke({\n",
    "    \"pipeline_path\": \"generated_code/merged_pipeline.py\",\n",
    "    \"csv_sample\": \"gesture_recognition.csv\"  # optional\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4d459755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class AutoCodeExecutor:\n",
      "    def __init__(self, code: str, max_attempts: int = 5):\n",
      "        self.original_code = code\n",
      "        self.code = code\n",
      "        self.max_attempts = max_attempts\n",
      "        self.llm = LLMWrapper(temperature=0.2).get_llm()\n",
      "\n",
      "    def try_execute(self):\n",
      "        attempt = 0\n",
      "\n",
      "        while attempt < self.max_attempts:\n",
      "            try:\n",
      "                print(f\"🛠 Attempt {attempt + 1}...\")\n",
      "                print(\"🔎 Current Code Preview:\\n\", self.code[:300])  # Preview first 300 chars\n",
      "\n",
      "                self.code = self.clean_code(self.code)  # clean each time\n",
      "                compiled_code = compile(self.code, \"<string>\", \"exec\")\n",
      "                exec(compiled_code, globals())\n",
      "                \n",
      "                print(\"✅ Code executed successfully!\")\n",
      "                return True  # Success\n",
      "            except Exception as e:\n",
      "                print(f\"❌ Error detected: {e}\")\n",
      "                error_message = traceback.format_exc()\n",
      "\n",
      "                # Ask LLM to fix the code\n",
      "                self.code = self.ask_llm_to_fix_code(self.code, error_message)\n",
      "\n",
      "                attempt += 1\n",
      "        \n",
      "        print(\"❌ Failed to fix code after maximum attempts.\")\n",
      "        return False  # Failed after retries\n",
      "\n",
      "\n",
      "    def clean_code(self, text):\n",
      "        text = text.strip()\n",
      "        text = re.sub(r\"^```(python)?\\n\", \"\", text)\n",
      "        text = re.sub(r\"^```\", \"\", text)\n",
      "        text = re.sub(r\"```$\", \"\", text)\n",
      "\n",
      "        first_line = text.split(\"\\n\")[0]              # ← fixed name\n",
      "        if not (\n",
      "            first_line.startswith((\"import\", \"from\", \"def\", \"class\"))\n",
      "        ):\n",
      "            text = \"\\n\".join(text.split(\"\\n\")[1:])\n",
      "\n",
      "        return text\n",
      "\n",
      "\n",
      "    def simplify_error(self,error_message) : \n",
      "        return error_message.strip().split(\"\\n\")[-1]\n",
      "\n",
      "\n",
      "    def ask_llm_to_fix_code(self, code: str, error_message: str) -> str:\n",
      "        simple_error = self.simplify_error(error_message)\n",
      "        \n",
      "        fix_prompt = f\"\"\"\n",
      "You are a Python expert.\n",
      "\n",
      "### Code:\n",
      "{code}\n",
      "\n",
      "### Error:\n",
      "{simple_error}\n",
      "\n",
      "Fix this Python script by resolving the error above.\n",
      "\n",
      "- Generate a fully corrected Python script.\n",
      "- Do NOT explain or describe anything.\n",
      "- Output ONLY fixed Python code.\n",
      "Start immediately with valid Python code.\n",
      "\"\"\"\n",
      "\n",
      "        fixed_code = self.llm.invoke(fix_prompt)\n",
      "\n",
      "        # If LLM response wrapped inside \"content\"\n",
      "        if hasattr(fixed_code, \"content\"):\n",
      "            fixed_code = fixed_code.content\n",
      "\n",
      "        print(\"🔧 LLM generated a fixed version!\")\n",
      "        return fixed_code.strip()\n",
      "\n",
      "\n",
      "    def save_fixed_code(self, filepath=\"generated_code/fixed_pipeline.py\"):\n",
      "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
      "        with open(filepath, \"w\") as f:\n",
      "            f.write(self.code)\n",
      "        print(f\"✅ Saved fixed code to: {filepath}\")\n",
      "\n",
      "    def ask_llm_to_fix_preprocess_function(self, code: str, error_message: str, \n",
      "                                       raw_code: str, \n",
      "                                       module_import: str = \"generated_code.merged_pipeline\") -> str:\n",
      "        \"\"\"\n",
      "        Fix a broken `preprocess(df)` function using helper functions from raw_code,\n",
      "        ensuring imports are used instead of copying full definitions.\n",
      "\n",
      "        Args:\n",
      "            code: The broken code to fix.\n",
      "            error_message: The exception that was raised.\n",
      "            raw_code: Full source code of helper functions (e.g., from merged_pipeline.py).\n",
      "            module_import: Module to import functions from.\n",
      "\n",
      "        Returns:\n",
      "            The fixed `preprocess(df)` code string.\n",
      "        \"\"\"\n",
      "        simple_error = self.simplify_error(error_message)\n",
      "\n",
      "        # Step 1: Extract helper function names\n",
      "        extract_prompt = f\"\"\"\n",
      "    The code below defines several helper functions used in data preprocessing.\n",
      "    Extract and return a comma-separated list of function names only.\n",
      "\n",
      "    Code:\n",
      "    {raw_code}\n",
      "    \"\"\"\n",
      "\n",
      "        extracted = self.llm.invoke(extract_prompt)\n",
      "        if hasattr(extracted, \"content\"):\n",
      "            extracted = extracted.content.strip()\n",
      "\n",
      "        function_names = extracted.replace(\"\\n\", \"\").strip()  # clean output like: \"scale, add_noise, encode_labels\"\n",
      "\n",
      "        # Step 2: Fix the broken code\n",
      "        fix_prompt = f\"\"\"\n",
      "    Fix the following broken Python function `preprocess(df)`, which failed with this error:\n",
      "\n",
      "    {simple_error}\n",
      "\n",
      "    If needed, import the following:\n",
      "    from {module_import} import {function_names}\n",
      "\n",
      "    Only return the corrected Python code. No comments. No explanations.\n",
      "\n",
      "    Broken code:\n",
      "    {code}\n",
      "    \"\"\"\n",
      "\n",
      "        fixed_code = self.llm.invoke(fix_prompt)\n",
      "        if hasattr(fixed_code, \"content\"):\n",
      "            fixed_code = fixed_code.content\n",
      "\n",
      "        print(\"🔁 LLM fixed preprocess function using external imports.\")\n",
      "        return fixed_code.strip()\n",
      "\n",
      "    def try_execute_preprocess_function(self, raw_code_path: str, module_import: str = \"generated_code.merged_pipeline\") -> bool:\n",
      "        \"\"\"\n",
      "        Attempt to execute the current `preprocess(df)` code.\n",
      "        If it fails, ask the LLM to fix it using external helper functions defined in a raw code file.\n",
      "\n",
      "        Args:\n",
      "            raw_code_path: Path to the original merged pipeline file for context.\n",
      "            module_import: Python import path (default: generated_code.merged_pipeline)\n",
      "\n",
      "        Returns:\n",
      "            True if successful, False after max retries.\n",
      "        \"\"\"\n",
      "        attempt = 0\n",
      "\n",
      "        # Load raw pipeline source code once\n",
      "        try:\n",
      "            with open(raw_code_path, \"r\", encoding=\"utf-8\") as f:\n",
      "                raw_code = f.read()\n",
      "        except Exception as e:\n",
      "            print(f\"❌ Failed to read raw code: {e}\")\n",
      "            return False\n",
      "\n",
      "        while attempt < self.max_attempts:\n",
      "            try:\n",
      "                print(f\"🛠 Attempt {attempt + 1}...\")\n",
      "                print(\"🔎 Current Code Preview:\\n\", self.code[:300])  # Preview first 300 chars\n",
      "\n",
      "                self.code = self.clean_code(self.code)\n",
      "                compiled_code = compile(self.code, \"<string>\", \"exec\")\n",
      "                exec(compiled_code, globals())\n",
      "\n",
      "                print(\"✅ preprocess(df) executed successfully!\")\n",
      "                return True\n",
      "            except Exception as e:\n",
      "                print(f\"❌ Error detected: {e}\")\n",
      "                error_message = traceback.format_exc()\n",
      "\n",
      "                # Ask LLM to fix using specialized logic\n",
      "                self.code = self.ask_llm_to_fix_preprocess_function(\n",
      "                    code=self.code,\n",
      "                    error_message=error_message,\n",
      "                    raw_code=raw_code,\n",
      "                    module_import=module_import\n",
      "                )\n",
      "\n",
      "                attempt += 1\n",
      "\n",
      "        print(\"❌ Failed to fix preprocess function after maximum attempts.\")\n",
      "        return False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(AutoCodeExecutor))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1ead37",
   "metadata": {},
   "source": [
    "## Generate Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c8c684e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tools' from 'c:\\\\Users\\\\Moham\\\\OneDrive\\\\Bureau\\\\Eurecom\\\\Semester_Project\\\\src\\\\tools.py'>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model_trainer_generator import ModelTrainerCodeGenerator\n",
    "import importlib\n",
    "import tools\n",
    "importlib.reload(tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e3e553ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Task: Classify between punch and flex using sensor data\\nFeatures:\\n  - aX: acceleration X\\n  - aY: acceleration Y\\n  - aZ: acceleration Z\\n  - gX: gyroscope X\\n  - gY: gyroscope Y\\n  - gZ: gyroscope Z\\nLabel Info: 0 for punch, 1 for flex\\nNotes: Gesture recognition with TinyML and sensor fusion\\nSample Data:\\n  - {'Unnamed: 0': 0, 'aX': 0.067, 'aY': 0.904, 'aZ': 2.018, 'gX': 70.435, 'gY': -10.315, 'gZ': 1.892, 'label': 'punch'}\\n  - {'Unnamed: 0': 1, 'aX': -0.045, 'aY': 1.269, 'aZ': 2.24, 'gX': 83.191, 'gY': -11.292, 'gZ': 9.583, 'label': 'punch'}\\n  - {'Unnamed: 0': 2, 'aX': -0.276, 'aY': 1.609, 'aZ': 2.364, 'gX': 94.849, 'gY': -5.249, 'gZ': 12.451, 'label': 'punch'}\\n  - {'Unnamed: 0': 3, 'aX': -0.564, 'aY': 2.0, 'aZ': 2.359, 'gX': 111.023, 'gY': 2.625, 'gZ': 7.874, 'label': 'punch'}\\n  - {'Unnamed: 0': 4, 'aX': -0.845, 'aY': 2.311, 'aZ': 2.335, 'gX': 137.878, 'gY': 10.681, 'gZ': -2.197, 'label': 'punch'}\""
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tools import load_context\n",
    "\n",
    "load_context.invoke({\n",
    "    \"csv_path\": \"gesture_recognition.csv\",\n",
    "    \"task_description\": \"Classify between punch and flex using sensor data\",\n",
    "    \"feature_desc_json\": '{\"aX\": \"acceleration X\", \"aY\": \"acceleration Y\", \"aZ\": \"acceleration Z\", \"gX\": \"gyroscope X\", \"gY\": \"gyroscope Y\", \"gZ\": \"gyroscope Z\"}',\n",
    "    \"label_desc\": \"0 for punch, 1 for flex\",\n",
    "    \"notes\": \"Gesture recognition with TinyML and sensor fusion\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "740eb701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'❌ Failed to import preprocess function: invalid syntax (generated_code/preprocess_module.py, line 1)'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tools import generate_training_code\n",
    "\n",
    "generate_training_code.invoke({\n",
    "    \"preprocess_module_path\": \"generated_code/preprocess_module.py\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3899eff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLMWrapper] Initialized Groq LLM: llama3-70b-8192 (temp=0.3)\n",
      "[LLMWrapper] Initialized Groq LLM: llama3-70b-8192 (temp=0.2)\n",
      "🛠 Attempt 1...\n",
      "🔎 Current Code Preview:\n",
      " ```\n",
      "import tensorflow as tf\n",
      "from tensorflow import keras\n",
      "from tensorflow.keras.models import load_model\n",
      "from tensorflow.lite import TFLiteConverter\n",
      "\n",
      "try:\n",
      "    model = load_model('models/final_model.h5', compile=False)\n",
      "    model.save('models/temp_model', save_format='tf')\n",
      "    converter = TFLiteConvert\n",
      "❌ Error detected: cannot import name 'TFLiteConverter' from 'tensorflow.lite' (c:\\Users\\Moham\\OneDrive\\Bureau\\Eurecom\\Semester_Project\\tiny_ml\\Lib\\site-packages\\tensorflow\\lite\\__init__.py)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Please consider providing the trackable_obj argument in the from_concrete_functions. Providing without the trackable_obj argument is deprecated and it will use the deprecated conversion path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 LLM generated a fixed version!\n",
      "🛠 Attempt 2...\n",
      "🔎 Current Code Preview:\n",
      " import tensorflow as tf\n",
      "\n",
      "try:\n",
      "    model = tf.keras.models.load_model('models/final_model.h5', compile=False)\n",
      "    model.save('models/temp_model', save_format='tf')\n",
      "    converter = tf.lite.TFLiteConverter.from_saved_model('models/temp_model')\n",
      "except Exception as e:\n",
      "    print(f\"Error occurred: {e}\")\n",
      "  \n",
      "Error occurred: The `save_format` argument is deprecated in Keras 3. Please remove this argument and pass a file path with either `.keras` or `.h5` extension.Received: save_format=tf\n",
      "✅ Code executed successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'✅ Model converted to TFLite and saved at models/final_model.tflite.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tools import convert_to_tflite_model\n",
    "convert_to_tflite_model.invoke({\n",
    "    \"original_model_path\": \"models/final_model.h5\",\n",
    "    \"converted_model_path\": \"models/final_model.tflite\",\n",
    "    \"input_datatype\": \"float32\",\n",
    "    \"output_datatype\": \"float32\",\n",
    "    \"quantization\": False\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5945b101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d58faf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e0fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0600ac03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceabe02f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiny_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
